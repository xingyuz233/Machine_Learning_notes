### 1.6 信息论（Information Theory）

信息量的度量$$h(x)$$应满足的性质

* $$h(x)$$ 与 $$p(x)$$有关，表示事件惊奇度。
* $$h(x,y)=h(x)+h(y) \quad \text{s.t.} \quad p(x,y)=p(x)p(y)$$，独立事件的信息量为两个事件的信息量只和。 

因此定义事件信息量度量$$h(x)$$为
$$
h(x)=-\log_2p(x)
$$
随机变量$$x$$的平均信息量或信息量期望，定义为随机变量$$x$$的**熵(entropy)**，单位为bit，熵是传输一个随机变量状态值所需最低比特位的下界。
$$
\mathrm{H}[x]=\mathbb{E}_x[h]=\sum_xp(x)h(x)=-\sum_xp(x)\log_2p(x)
$$
熵也可以以e为对数底数，此时度量单位为nat
$$
\mathrm{H}[x]=-\sum_xp(x)\ln p(x)
$$
设随机变量$$x$$的状态表示为$$x_i(i=1,...,M)$$，熵的最大化问题转化为约束条件下的优化问题
$$
\begin{gathered}
\mathrm{H}=-\sum_{i=1}^{M}p(x_i)\ln p(x_i) \\
\sum_{i=1}^{M}p(x_i)=1
\end{gathered}
$$
使用拉格朗日乘子法进行问题转化优化函数
$$
\widetilde{\mathrm{H}}=-\sum_{i=1}^{M}p(x_i)\ln p(x_i)+\lambda(\sum_{i=1}^{M}p(x_i)-1)
$$
对熵求二阶导数，可以得到
$$
\frac{\partial^2\widetilde{\mathrm{H}}}{\partial p(x_i)\partial p(x_j)}=-I_{i=j}\frac{1}{p_i}
$$
熵的Hessian矩阵为半负定的，因此驻点即为最大值，设$$\nabla_{p(x_i),\lambda}\widetilde{\mathrm{H}}=\mathbf{0}$$，得到下面的等式
$$
\begin{gathered}
\ln p(x_i)-1+\lambda=0 \quad \text{for }i=1,...,M \\
\sum_{i=1}^{M}p(x_i)=1

\end{gathered}
$$
得到使得$$\widetilde{\mathrm{H}}$$最大化的最优解为
$$
\begin{gathered}
p(x_i)^*= \frac{1}{M}\\
\mathrm{H}^*=\ln M
\end{gathered}
$$
对于连续随机变量$$x$$的情形，设$$p(x)$$是连续的，将$$x$$划分为无穷多个等宽区域$$\Delta \to 0$$，将第$$i$$个区域的概率密度设为$$p(x_i)$$。
$$
\int_{i\Delta}^{(i+1)\Delta}p(x)\mathrm{d}x=p(x_i)\Delta
$$
求得熵的形式为
$$
\begin{aligned}
\mathrm{H}_\Delta
&=-\sum_i(p(x_i)\Delta)\ln(p(x_i)\Delta) \\
&=-\sum_ip(x_i)\Delta\ln p(x_i)-\ln\Delta \\
&=-\int p(x)\ln p(x)\mathrm{d}x -\ln\Delta 
\end{aligned}
$$
定义微分熵为
$$
\mathrm{H}[\mathbf{x}]=-\int p(\mathbf{x})\ln p(\mathbf{x})\mathrm{d}\mathbf{x}
$$
对于连续变量最大熵问题，添加归一化，一阶矩，二阶矩的约束
$$
\begin{gathered}
\int_{-\infty}^{+\infty}p(x)\mathrm{d}x=1 \\
\mathbb{E}[x]=\int_{-\infty}^{+\infty}xp(x)\mathrm{d}x=\mu \\
\mathrm{var}[x]=\int_{-\infty}^{+\infty}(x-\mu)^2p(x)\mathrm{d}x=\sigma^2 \\
\end{gathered}
$$
使用拉格朗日乘数法求解，最优化下面关于$$p(x)$$的泛函数
$$
\widetilde{\mathrm{H}}[p]=-\int_{-\infty}^{+\infty}p(x)\ln p(x)\mathrm{d}x+\lambda_1(\int_{-\infty}^{+\infty}p(x)\mathrm{d}x-1) \\
+\lambda_2(\int_{-\infty}^{+\infty}xp(x)\mathrm{d}x-\mu)+\lambda_3(\int_{-\infty}^{+\infty}(x-\mu)^2p(x)\mathrm{d}x-\sigma^2)
$$

* 对$$p(x)$$使用变分法，得到下式
  $$
  \begin{gathered}
  -\ln p(x)-1 + \lambda_1 + \lambda_2x + \lambda_3(x-\mu)^2 = 0 \quad \text{for all }x \\
  \end{gathered}
  $$

* 设关于$$\lambda_1,\lambda_2,\lambda_3$$的偏导数为0，得到上式的三个约束

* 联立可以求得最优$$p(x)$$与最大化的熵$$\mathrm{H}[x]$$
  $$
  \begin{gathered}
  p(x)=\frac{1}{\sqrt{2\pi}\sigma}\exp\{-\frac{(x-\mu)^2}{2\sigma^2}\} \\
  \mathrm{H}[x] = \frac{1}{2}\{1+\ln(2\pi\sigma^2)\}
  \end{gathered}
  $$

最大化微分熵的分布为高斯分布，这种熵随着分布宽度$$\sigma$$的增加而增加

定义联合概率分布$$p(\mathbf{x},\mathbf{y})$$，随机变量$$\mathbf{y}$$关于$$\mathbf{x}$$的条件微分熵定义为$$\mathrm
{H}[\mathbf{y}|\mathbf{x}]$$，表示为$$\mathbf{y}$$在所有$$\mathbf{x}$$下的平均信息量
$$
\mathrm{H}[\mathbf{y}|\mathbf{x}]=-\int p(\mathbf{x})\int p(\mathbf{y}|\mathbf{x})\ln p(\mathbf{y|\mathbf{x}})\mathrm{d}\mathbf{y}\mathrm{d}\mathbf{x}
=-\iint p(\mathbf{y},\mathbf{x})\ln p(\mathbf{y|\mathbf{x}})\mathrm{d}\mathbf{y}\mathrm{d}\mathbf{x}
$$
定义联合分布$$p(\mathbf{x},\mathbf{y})$$的微分熵为$$\mathrm{H}[\mathbf{x},\mathbf{y}]$$，通过下式可以看出，描述$$\mathbf{x},\mathbf{y}$$所需的信息是描述$$\mathbf{x}$$自己所需的信息，加上给定$$\mathbf{x}$$的情况下具体化$$\mathbf{y}$$所需的额外信息 。
$$
\begin{aligned}
\mathrm{H}[\mathbf{x},\mathbf{y}]
&=\iint p(\mathbf{x},\mathbf{y})\ln p(\mathbf{x},\mathbf{y})\mathrm{d}\mathbf{x}\mathrm{d}\mathbf{y} \\
&=\iint p(\mathbf{x},\mathbf{y})(\ln p(\mathbf{y}|\mathbf{x})+\ln p(\mathbf{x}))\mathrm{d}\mathbf{x}\mathrm{d}\mathbf{y} \\
&=\iint p(\mathbf{x},\mathbf{y})\ln p(\mathbf{y}|\mathbf{x})\mathrm{d}\mathbf{x}\mathrm{d}\mathbf{y}+\int p(\mathbf{x})\ln p(\mathbf{x})\mathrm{d}\mathbf{x} \\
&=\mathrm{H}[\mathbf{y}|\mathbf{x}]+\mathrm{H}[\mathbf{x}]
\end{aligned}
$$

#### 1.6.1 相对熵与互信息

设存在未知分布$$p(\mathbf{x})$$，若使用分布$$q(\mathbf{x})$$对其进行建模，建立起来的编码体系所需要的平均附加信息量定义为分布$$p(\mathbf{x})$$和分布$$q(\mathbf{x})$$的**相对熵(relative entropy)**或**KL散度(KL-Divergence)**
$$
\begin{aligned}
\mathrm{KL}(p \parallel q) 
&= -\int p(\mathbf{x})\ln q(\mathbf{x})\mathrm{d}\mathbf{x}-(-\int p(\mathbf{x})\ln p(\mathbf{x})\mathrm{d}\mathbf{x}) \\
&= -\int p(\mathbf{x})\ln\frac{q(\mathbf{x})}{p(\mathbf{x})}\mathrm{d}\mathbf{x}
\end{aligned}
$$
接下来将证明KL散度满足$$\mathrm{KL}(p \parallel q)\ge0$$，当且仅当$$p(\mathbf{x})=q(\mathbf{x})$$时等号成立。

对于凸函数而言，任意两点间的弦都位于图像的上方，即
$$
f(\lambda a+(1-\lambda)b)\le \lambda f(a)+(1-\lambda)f(b) \quad \
text{s.t. }0\le\lambda\le1  
$$
当等号仅在$$\lambda=0$$或$$\lambda=1$$时取得时，该凸函数称为严格的，通过对上式进行归纳，可以得出凸函数的一个更一般的性质，即**Jensen不等式**
$$
f\bigg(\sum_{i=1}^{M}\lambda_ix_i\bigg)\le\sum_{i=1}^{M}\lambda_if(x_i) \quad \text{s.t. }\lambda_i\ge0,\sum_{i=1}^{M}\lambda_i=1
$$
由于概率分布$$p(\mathbf{x})$$的归一性和非负性，即满足$$\lambda$$的条件，对概率分布应用Jensen不等式，得到
$$
f(\mathbb{E}[\mathbf{x}])\le\mathbb{E}[f(\mathbf{x})]
$$
对应于上式的离散型和连续型变量分别的形式为
$$
\begin{gathered}
f\bigg(\sum_{i=1}^{M}\mathbf{x}_ip(\mathbf{x}_i) \bigg) \le \sum_{i=1}^{M}p(\mathbf{x}_i)f(\mathbf{x}_i) \\
f\bigg(\int\mathbf{x}p(\mathbf{x})\mathrm{d}\mathbf{x}\bigg)\le \int p(\mathbf{x})f(\mathbf{x})\mathrm{d}\mathbf{x}
\end{gathered}
$$
利用$-\ln x$的严格凸性，可以得到$$\mathrm{KL}(p \parallel q)\ge0$$，当且仅当$$p(\mathbf{x})=q(\mathbf{x})$$时等号成立。
$$
\begin{aligned}
\mathrm{KL}(p\parallel q)
&=-\int p(\mathbf{x})\ln\frac{q(\mathbf{x})}{p(\mathbf{x})}\mathrm{d}\mathbf{x} \\
&=\int p(\mathbf{x})\big(-\ln\frac{q(\mathbf{x})}{p(\mathbf{x})}\big)\mathrm{d}\mathbf{x} \\
&\ge -\ln\bigg(\int p(\mathbf{x})\frac{q(\mathbf{x})}{p(\mathbf{x})}\mathrm{d}\mathbf{x} \bigg) \\
&=-\ln\bigg(\int q(\mathbf{x})\mathrm{d}\mathbf{x} \bigg) \\
&=0
\end{aligned}
$$
对于未知分布$$p(\mathbf{x})$$求参数分布$$q(\mathbf{x}|\pmb{\theta})$$的最佳近似，可以通过有限数量的训练点$$\mathbf{x}_n$$近似KL散度，通过最小化KL散度来对$$\pmb{\theta}$$做点估计。
$$
\mathrm{KL}(p \parallel q) \simeq\frac{1}{N}\sum_{n=1}^N\{-\ln q(\mathbf{x}_n|\pmb{\theta})+\
ln p(\mathbf{x}_n)\}
$$
$$\mathrm{KL}$$散度还可以衡量两个分布之间的不相似性，比如利用$$p(x,y)$$与$$p(x)p(y)$$之间的KL散度来描述$$x,y$$之间的相关性或互信息，定义变量$$\mathbf{x},\mathbf{y}$$之间的**互信息(mutual information)**为
$$
\begin{aligned}
\mathrm{I}[\mathbf{x},\mathbf{y}]
&\equiv\mathrm{KL}(p(\mathbf{x},\mathbf{y})\parallel p(\mathbf{x})p(\mathbf{y})) \\
&=-\iint p(\mathbf{x},\mathbf{y})\frac{p(\mathbf{x})p(\mathbf{y})}{p(\mathbf{x},\mathbf{y})}\mathrm{d}\mathbf{x}\mathrm{d}\mathbf{y} \\
\end{aligned}
$$
显然，$$\mathrm{I}[\mathbf{x},\mathbf{y}] \ge 0$$，当且仅当$$\mathbf{x}$$和$$\mathbf{y}$$相互独立时等号成立。互信息与条件熵之间的关系为
$$
\mathrm{I}[\mathbf{x},\mathbf{y}]=\mathrm{H}[\mathbf{x}]-\mathrm{H}[\mathbf{x}|\mathbf{y}]=\mathrm{H}[\mathbf{y}]-\mathrm{H}[\mathbf{y}|\mathbf{x}]
$$
可以看出，互信息可以形式化的描述为一个新的观测$$\mathbf{y}$$造成的$$\mathbf{x}$$的不确定性的减小（反之亦然）。 

