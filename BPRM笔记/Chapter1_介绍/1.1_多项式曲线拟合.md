### 1.1 例子：多项式曲线拟合（Example: Polynomial Curve Fitting）

使用多项式函数进行曲线拟合
$$
y(x,\mathbf{w})=w_0+w_1x+w_2x^2+...+w_Mx^M=\sum_{j=0}^Mw_jx^j
$$
函数$$y(x,\mathbf{w})$$是系数$$\mathbf{w}$$的线性函数，叫做**线性模型**。

定义平方和误差函数
$$
E(\mathbf{w})=\frac{1}{2}\sum_{n=1}^{N}\{y(x_n,\mathbf{w})-t_n\}^2
$$
曲线拟合问题转化为最小化误差函数问题
$$
\mathbf{w}^*=\arg\min_\mathbf{w}E(\mathbf{w})
$$
对于测试集的误差，一般使用根均方(RMS)误差表示
$$
E_{RMS}=\sqrt{2E(\mathbf{w^*})/N}
$$
添加L2正则项避免过拟合，使用L2正则化的方法也叫做**岭回归(Ridge Regression)**
$$
\widetilde{E}(\mathbf{w})=\frac{1}{2}\sum_{n=1}^{N}\{y(x_n,\mathbf{w})-t_n\}^2+\lambda\mathbf{w}^\mathrm{T}\mathbf{w}
$$

#### 练习

* 考虑平方和误差函数$$E(\mathbf{w})=\frac{1}{2}\sum_{n=1}^{N}\{y(x_n,\mathbf{w})-t_n\}^2$$，其中函数$$y(x_n,\mathbf{w})=\sum_{j=0}^Mw_jx_n^j$$。 证明最小化误差函数的系数$$w=\{w_i\}$$由线性方程的集合给出$$\sum_{j=0}^{M}A_{ij}w_j=T_i$$，其中$$A_{ij}=\sum_{n=1}^N(x_n)^{i+j}, T_i=\sum_{n=1}^{N}(x_n)^it_n$$。

  由于$E(\mathbf{w})$的二阶导数$$\frac{\partial E}{\partial{w_i}\partial{w_j}}
  = \sum_{n=1}^{N}x_n^{i+j}>0$$，所以$$E(\mathbf{w})$$的Hessian矩阵满足正定性，即梯度为0时取得最小值：
  $$
  \nabla_\mathbf{w}E(\mathbf{w})=\sum_{n=1}^N\bigg(\big(y(x_n,\mathbf{w})-t_n\big)[1,x_n,...,x_n^M]^\mathrm{T}\bigg)=\mathbf{0}
  $$
  可以得到
  $$
  \begin{gathered}
  \sum_{n=1}^N\sum_{j=0}^{M}w_jx_n^{j+i}=\sum_{n=1}^{N}t_nx_n^i \quad \text{for all }i\le M
  \end{gathered}
  $$
  
* 写下能够使由$$\widetilde{E}(\mathbf{w})=\frac{1}{2}\sum_{n=1}^{N}\{y(x_n,\mathbf{w})-t_n\}^2+\lambda\mathbf{w}^\mathrm{T}\mathbf{w}$$给出的正则化的平方和误差函数取得最小值的系数$$w_i$$应该满足的与上面小题类似的一组线性方程。

  由于$$E(\mathbf{w})$$的二阶导数$$\frac{\partial E}{\partial{w_i}\partial{w_j}}
  = \sum_{n=1}^{N}x_n^{i+j}+\lambda\mathrm{I}_{i=j} >0$$，所以$$E(\mathbf{w})$$的Hessian矩阵满足正定性，即梯度为0时取得最小值：
  $$
  \nabla_\mathbf{w}E(\mathbf{w})=\sum_{n=1}^N\bigg(\big(y(x_n,\mathbf{w})-t_n\big)[1,x_n,...,x_n^M]^\mathrm{T}\bigg)+\mathbf{w}=\mathbf{0}
  $$
  可以得到
  $$
  \begin{gathered}
  \sum_{n=1}^N\sum_{j=0}^{M}w_jx_n^{j+i}+w_i=\sum_{n=1}^{N}t_nx_n^i \quad \text{for all }i\le M
  \end{gathered}
  $$
  化简为
  $$
  \begin{gathered}
  \sum_{j=0}^MA_{ij}w_j+w_i=T_i \\
  A_{ij}=\sum_{n=1}^N(x_n)^{j+i} \\
  T_i=\sum_{n=1}^N(x_n)^it_n
  \end{gathered}
  $$
  