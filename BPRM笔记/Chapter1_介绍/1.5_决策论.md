### 1.5 决策论（Decision Theory）

对于分类问题而言，考虑一个医疗诊断问题，我们根据病人拍的X光片，来判断病人是否患有癌症。设输入变量$\mathbf{x}$表示X光片的灰度值集合，输出变量$t$表示病人是否患有癌症（$t=0$，记作类$\mathcal{C}_1$）或者不患癌症（$t=1$，记作类$\mathcal{C}_2$）。从训练集中确定联合概率分布$p(\mathbf{x},\mathcal{C}_k)$或$p(\mathbf{x},t)$的问题叫做**推断问题(inference problem)**，根据推断问题的概率描述，针对于输入进行决策的问题（如确定病人是否需要治疗）称为**决策问题(decision problem)**

根据贝叶斯定理，将输入图像$\mathbf{x}$分类为$\mathcal{C}_k$的概率可以表示成
$$
p(\mathcal{C_k}|\mathbf{x})=\frac{p(\mathbf{x}|\mathcal{C}_k)p(\mathcal{C}_k)}{p(\mathbf{x})}
$$
$p(\mathcal{C_k})$称为类$\mathcal{C}_k$的先验概率，表示拍X光之前，一个人是否患癌症的概率。$p(\mathcal{C}_k|\mathbf{x})$表示类$\mathcal{C_k}$的后验概率。

#### 1.5.1 最小化错误分类率（Minimizing the misclassfication rate）

我们的目标是尽可能少地作出错误的分类。首先通过一个规则把每个$\mathbf{x}$分到一个合适的类别，这种规则将会把输入空间切分成不同的区域$\mathcal{R}_k$ ，这种类别称为**决策区域(decision boundary)**。每个类别都有一个决策区域，区域$\mathcal{R}_k$中的所有点都被分到$\mathcal{C}_k$类。决策区域间的边界被叫做**决策边界(decision boundary)**或者**决策面(decision surface)**。

考虑只有两类的情形，即$k=1,2$，作出错误分类的概率为
$$
\begin{aligned}
p(\text{mistake})
&=p(\mathbf{x}\in \mathcal{R}_1,\mathcal{C}_2)+p(\mathbf{x}\in \mathcal{R}_2, \mathcal{C}_1) \\
&=\int_{\mathcal{R}_1}p(\mathbf{x},\mathcal{C}_2)+\int_{\mathcal{R}_2}p(\mathbf{x},\mathcal{C_1})
\end{aligned}
$$
为了最小化错误分类概率，将输入$\mathbf{x}$

使得错误分类率最低的决策规则将输入$\mathbf{x}$进行如下的指派
$$
\mathbf{x} \in \begin{cases}
	\mathcal{R}_1 & \text{if } p(\mathbf{x},\mathcal{C}_1) > p(\mathbf{x},\mathcal{C}_2) \\
  \mathcal{R}_2 & \text{else}
\end{cases}
$$
得到的决策区域为
$$
\begin{gathered}
\mathcal{R}_1=\{\mathbf{x}|p(\mathbf{x},\mathcal{C}_1
)>p(\mathbf{x},\mathcal{C}_2)\} \\
\mathcal{R}_2=\{\mathbf{x}|p(\mathbf{x},\mathcal{C}_1
)\le p(\mathbf{x},\mathcal{C}_2)\} \\
\end{gathered}
$$
对于多类情形，一般使用最大化正确率的方法，即最大化下式
$$
p(\text{correct})=\sum_{k=1}^K p(\mathbf{x}\in \mathcal{R}_k,\mathcal{C}_k)=\sum_{k=1}^{K}\int_{\mathcal{R}_k}p(\mathbf{x},\mathcal{C}_k)d\mathbf{x}
$$
同理使得正确率得到最大化的决策规则将输入$\mathbf{x}$指派到$\mathcal{R}_{k^*}$中
$$
\begin{gathered}
\mathbf{x}\in\mathcal{R}_{k^*} \\
k^*=\arg\max_k p(\mathbf{x},\mathcal{C}_k)=\arg\max_k p(\mathcal{C}_k|\mathbf{x})p(\mathbf{x})=\arg\max_k p(\mathcal{C}_k|\mathbf{x})
\end{gathered}
$$
得到的决策区域为
$$
\mathcal{R}_k=\{\mathbf{x}|k=\arg\max_{k'}p(\mathcal{C}_{k'}|\mathbf{x})\}
$$

#### 1.5.2 最小化期望损失（Minimizing the expected loss）

定义**损失矩阵(Loss Matrix)**$L$， 其中$L_{kj}$表示将真实类别为$\mathcal{C}_k$的$\mathbf{x}$分类为$\mathcal{C}_j$所造成的损失。

由于损失函数依赖于真实的类别，而真实的类别是未知的，我们是通过联合概率分布$p(\mathbf{x},\mathcal{C}_k)$来表示真实类别的不确定性的，通过对真实类别的不确定性的联合概率分布描述，定义**平均损失(average loss)**或者**期望损失(expected loss)**为
$$
\mathbb{E}[L]=\sum_k\sum_j\int_{\mathcal{R}_j}L_{kj}p(\mathbf{x},\mathcal{C}_k)\mathrm{d}\mathbf{x}
$$
使得期望损失最小化的最优决策规则满足
$$
\begin{gathered}
\mathbf{x}\in\mathcal{R}_{j^*} \\
j^* = \arg\min_j\sum_k L_{kj}p(\mathbf{x}, \mathcal{C}_k)=\arg\min_j\sum_k L_{kj}p(\mathcal{C}_k|\mathbf{x})
\end{gathered}
$$
得到的决策区域为
$$
\mathcal{R}_{j}=\{\mathbf{x}|j=\arg\min_{j'}\sum_k L_{kj'}p(\mathcal{C_k}|\mathbf{x})\}
$$

#### 1.5.3 拒绝选择（The reject option）

对于$p(\mathcal{C}_k|\mathbf{x})$比较接近的地方，类别的归属相对不确定，为了使得预测错误率降低，可以拒绝对$\mathbf{x}$进行预测，这样的方法称为**拒绝选择**，可以通过引入阈值$\theta$来定义**拒绝域(reject region)**
$$
\mathcal{R}_{reject}=\{\mathbf{x}|\max_kp(\mathcal{C}_k|\mathbf{x})<\theta \}
$$

* $\theta=1$，所有样本都被拒绝
* $\theta < \frac{1}{K}$，$K$为类别数，确保没有样本被拒绝

#### 1.5.4 推断与决策（Inference and decision）

上述例子将分类问题划分成了**推断(inference)**阶段和**决策(decision)**阶段，在推断阶段我们使用训练阶段学习后验概率分布$p(\mathcal{C}_k|\mathbf{x})$，在决策阶段利用这些后验概率分布来进行最优的分类。解决决策的三类方法包括**生成模型(Generative Model)，判别模型(Discriminative Model)，判别函数(Discriminant Function)**

* 生成模型

  * 对类条件密度$p(\mathbf{x}|\mathcal{C}_k)$和先验类概率$p(\mathcal{C}_k)$建模形成推断问题，根据贝叶斯定理求出后验概率，然后进行决策问题
    $$
    p(\mathcal{C}_k|\mathbf{x})=\frac{p(\mathbf{x}|\mathcal{C}_k)p(\mathcal{C}_k)}{p(\mathbf{x})}=\frac{p(\mathbf{x}|\mathcal{C}_k)p(\mathcal{C}_k)}{\sum_k p(\mathbf{x}|\mathcal{C}_k)p(\mathcal{C}_k)}
    $$

  * 或者直接对联合概率分布$p(\mathbf{x},\mathcal{C}_k)$建模形成推断问题，归一化求出后验概率，然后进行决策问题
    $$
    p(\mathcal{C}_k|\mathbf{x})=\frac{p(\mathbf{x},\mathcal{C}_k)}{p(\mathbf{x})}=\frac{p(\mathbf{x},\mathcal{C}_k)}{\sum_k p(\mathbf{x},\mathcal{C}_k)}
    $$

  生成模型需要大量的进行求解计算过程，但优点是可以求出边缘概率密度$p(\mathbf{x})$，可以用来检测低概率点，即**离群检测(outlier detection)**或者**异常检测(novelty detection)**

* 判别模型

  直接对后验类密度$p(\mathcal{C}_k|\mathbf{x})$建模形成推断问题，然后进行决策问题。

  若只需进行分类决策，则生成模型过于浪费，可采用判别模型。

* 判别函数

  找到一个函数$f(\mathbf{x})$，被称为判别函数。这个函数把每个输入$\mathbf{x}$直接映射为类别标签。例如，在二分类问题中，$f(·)$可能是一个二元的数值，$f = 0$表示类别$\mathcal{C}_1$，$f = 1$表示类别$\mathcal{C}_2$。这种情况下，概率不起作用。

  判别函数中，我们无法获得后验概率，而后验概率在“拒绝选择”，“最小化风险”， “模型组合”等诸多领域都有用。

#### 1.5.5 回归问题的损失函数（Loss functions for regression）

定义回归的损失函数$L$，$L(t,y(\mathbf{x}))$表示对于输入$\mathbf{x}$，输出为$t$的样本，其估计$y(\mathbf{x})$所造成的损失。则回归问题的期望损失可以表示成
$$
\mathbb{E}[L]=\int\int L(t,y(\mathbf{x}))p(\mathbf{x},t)\mathrm{d}\mathbf{x}\mathrm{d}t
$$
若选择损失函数为平方损失，即$L(t,y(\mathbf{x})=\{y(\mathbf{x})-t \}^2$，则上式转化为
$$
\mathbb{E}[L]=\int\int \{y(\mathbf{x})-t \}^2p(\mathbf{x},t)\mathrm{d}\mathbf{x}\mathrm{d}t
$$
##### 变分法求解最优$y(\mathbf{x})$

我们的目标是选择$y(\mathbf{x})$来最小化$\mathbb{E}[L]$。如果我们假设一个完全任意的函数$y(\mathbf{x})$，我们能够形式化地使用变分法求解：

* 根据*Fubini's theorem*，对$E[L]$交换积分顺序
  $$
  \mathbb{E}[L] = \int\int\{y(\mathbf{x})-t\}^2p(\mathbf{x},t)\mathrm{d}t\mathrm{d}\mathbf{x}
  $$

* 取$G(y,y',x)=\int\{y(\mathbf{x}-t)\}^2p(\mathbf{x},t)\mathrm{d}t$，根据欧拉-拉格朗日方程可以求出使得$\mathbb{E}[L]$最小的函数$y(\mathbf{x})$
  $$
  \begin{gathered}
  \frac{\part G}{\part y}-\frac{\mathrm{d}}{\mathrm{d}x}(\frac{\part G}{\part y'})=\frac{\part G}{\part y}=2\int\{y(\mathbf{x})-t \}p(\mathbf{x},t)\mathrm{d}t=0 \\
  y(\mathbf{x})=\frac{\int tp(\mathbf{x},t)\mathrm{d}t}{\int p(\mathbf{x},t)\mathrm{d}t}=\frac{\int tp(\mathbf{x},t)\mathrm{d}t}{p(\mathbf{x})}=\int tp(t|\mathbf{x})\mathrm{d}t=\mathbb{E}_t(t|\mathbf{x})
  \end{gathered}
  $$

##### 配方法求解最优$y(\mathbf{x})$

从另一个角度看，如果已知最优解为$\mathbb{E}_t(t|\mathbf{x})$，可以对$\{y(\mathbf{x}-t)\}^2$展开
$$
\begin{aligned}
\{y(\mathbf{x})-t\}^2
&=\{y(\mathbf{x})-\mathbb{E}_t[t|\mathbf{x}]+\mathbb{E}_t[t|\mathbf{x}]-t\}^2 \\
&=\{y(\mathbf{x})-\mathbb{E}_t[t|\mathbf{x}]\}^2+2\{y(\mathbf{x})-\mathbb{E}_t[t|\mathbf{x}]\}\{\mathbb{E}_t[t|\mathbf{x}]-t\}+\{\mathbb{E}_t[t|\mathbf{x}]-t\}^2 \\
&= m(\mathbf{x})^2+2m(\mathbf{x})n(\mathbf{x},t)+n(\mathbf{x},t)^2
\end{aligned}
$$
将上式代入$E[L]$得
$$
\begin{aligned}
\mathbb{E}[L]
&=\int\int(y(\mathbf{x})-t)^2p(\mathbf{x},t)\mathrm{d}t\mathrm{d}\mathbf{x} \\
&=\int\int(m(\mathbf{x})^2+2m(\mathbf{x})n(\mathbf{x},t)+n(\mathbf{x},t)^2))p(\mathbf{x},t)\mathrm{d}t\mathrm{d}\mathbf{x} \\

&=\int m(\mathbf{x})^2p(\mathbf{x})\mathrm{d}\mathbf{x} + 2\int m(\mathbf{x})(\int\mathbb{E}_t[t|\mathbf{x}]p(\mathbf{x},t)\mathrm{d}t-\int tp(\mathbf{x},t)\mathrm{d}t)\mathrm{d}\mathbf{x}+\int\int n(\mathbf{x},t)^2p(\mathbf{x},t)\mathrm{d}t\mathrm{d}\mathbf{x}  \\
&=\int m(\mathbf{x})^2p(\mathbf{x})\mathrm{d}\mathbf{x} + 2\int m(\mathbf{x})(\mathbb{E}_t[t|\mathbf{x}]\int p(\mathbf{x},t)\mathrm{d}t-p(\mathbf{x})\int tp(t|\mathbf{x})\mathrm{d}t)\mathrm{d}\mathbf{x}+\int\int n(\mathbf{x},t)^2p(\mathbf{x},t)\mathrm{d}t\mathrm{d}\mathbf{x}  \\
&=\int m(\mathbf{x})^2p(\mathbf{x})\mathrm{d}\mathbf{x} + 2\int m(\mathbf{x})(\mathbb{E}_t[t|\mathbf{x}]p(\mathbf{x})-p(\mathbf{x})\mathbb{E}_t[t|\mathbf{x}])\mathrm{d}\mathbf{x}+\int\int n(\mathbf{x},t)^2p(\mathbf{x},t)\mathrm{d}t\mathrm{d}\mathbf{x}  \\
&=\int m(\mathbf{x})^2p(\mathbf{x})\mathrm{d}\mathbf{x}+\int\int n(\mathbf{x},t)^2p(\mathbf{x},t)\mathrm{d}t\mathrm{d}\mathbf{x}  \\
&= \int\{y(\mathbf{x})-\mathbb{E}_t[t|\mathbf{x}]\}^2\mathrm{d}\mathbf{x}+\int p(\mathbf{x})\int\{\mathbb{E}_t[t|\mathbf{x}]-t\}^2p(t|\mathbf{x})\mathrm{d}t\mathrm{d}\mathbf{x} \\
&= \int\{y(\mathbf{x})-\mathbb{E}_t[t|\mathbf{x}]\}^2\mathrm{d}\mathbf{x}+\int \mathrm{var}[t|\mathbf{x}] p(\mathbf{x})\mathrm{d}\mathbf{x} \\
\end{aligned}
$$
第二项与$y(\mathbf{x})$无关，因此只需最小化第一项，得到$y$的最优解，以及最小损失
$$
\begin{gathered}
y^*(\mathbf{x})=\mathbb{E}_t[t|\mathbf{x}] \\
\mathbb{E}^*[L]=\int\mathrm{var}[t|\mathbf{x}]p(\mathbf{x})\mathrm{d}\mathbf{x}
\end{gathered}
$$

##### 求解回归问题决策的三种方法

* 对$p(\mathbf{x},t)$形成推断问题（这里与分类方法不同的是，回归问题很难推断$p(\mathbf{x}|t)$的情形），计算后验概率$p(t|\mathbf{x})$，进行决策（如最小化平方损失函数得到的决策$y(\mathbf{x})=\mathbb{E}[t|\mathbf{x}]$）。
* 对$p(\mathbf{x}|t)$形成推断问题，进行决策
* 直接对回归函数$y(\mathbf{x})$建模

##### 损失函数的推广

闵可夫斯基损失函数(Minkowski loss)的期望表示为
$$
\mathbb{E}[L_q]=\int\int\left|y(\mathbf{x}-t)\right|^qp(\mathbf{x},t)\mathrm{d}\mathbf{x}\mathrm{d}t
$$

* $q=1$时，$\arg\min_y\mathbb{E}[L_1]$为条件中位数

* $q=2$时，$\arg\min_y\mathbb{E}[L_2]$为条件均值

* $q \to 0$时，$\arg\min_y\mathbb{E}[L_q]$为条件众数

