### 1.2 概率论（Probability Theory）

设离散型随机变量$$X,Y$$，$$X$$取值为$$\{x_i\}_{i=1}^{M}$$，$$Y$$的取值为$$\{y_j\}_{j=1}^{L}$$，对随机变量进行$$N$$次采样，设$$N$$次采样中满足$$X=x_i,Y=y_j$$的样本个数为$$n_{ij}$$，满足$$X=x_i$$的样本个数为$$c_i$$，满足$$Y=y_j$$的样本个数为$$r_j$$。

$$X,Y$$的**联合概率（joint probability）**表示为
$$
p(X=x_i,Y=y_j)=\frac{n_{ij}}{N} \tag{1.2.1}
$$
$$X,Y$$分别的**边缘概率（marginal probability）**表示为
$$
\begin{gathered}
p(X=x_i)=\sum_{j=1}^{L}P(X=x_i, Y= y_j) = \frac{c_i}{N} \\
p(Y=y_j)=\sum_{i=1}^{M}P(X=x_i, Y=y_j) = \frac{r_j}{N} 
\end{gathered}
\tag{1.2.2}
$$
$$Y$$关于$$X$$的**条件概率（conditional probability）**表示为
$$
p(Y=y_j|X=x_i) = \frac{p(Y=y_j,X=x_i)}{P(X=x_i)} =\frac{n_{ij}}{c_i} \tag{1.2.3}
$$
概率的**加法法则（sum rule）**表示为
$$
p(X) = \sum_Yp(X,Y) \tag{1.2.4}
$$
概率的**乘法法则（product rule）**表示为
$$
p(X,Y)=p(Y|X)p(X)=p(X|Y)p(Y) \tag{1.2.5}
$$
根据乘法法则进行变形可以得到**贝叶斯理论（Bayes' theorem）**
$$
p(Y|X) = \frac{p(X|Y)p(Y)}{p(X)} \tag{1.2.6}
$$
利用加法法则可以将（1.2.6）的分母改写成累和的形式，分母也可以看作是使$$p(Y|X)$$对所有$Y$取值进行归一化的一个超参数。
$$
p(Y|X) = \frac{p(X|Y)p(Y)}{\sum_Yp(X|Y)p(Y)} \tag{1.2.7}
$$
随机变量$$X,Y$$是相互独立的，当且仅当
$$
p(X,Y)=p(X)p(Y) \tag{1.2.8}
$$

#### 1.2.1 概率密度（Probability densities）

对于连续型的随机变量$$X$$而言，对于$$\delta{x}\to 0$$, 设变量$$X$$落在区间$$(x, x+\delta x)$$的概率为$$p(x)\delta x$$，称$$p(x)$$为连续型随机变量$$X$$在点$$x$$处的**概率密度（probability density）**。随机变量$$X$$落在$$
(a,b)$$区间上的概率计算公式为
$$
p(x\in(a,b))=\int_a^bp(x)dx \tag{1.2.9}
$$
概率密度$$p(x)$$满足非负性和归一性
$$
\begin{gathered}
p(x) \ge 0 \\
\int_a^bp(x)dx = 1 
\end{gathered}
\tag{1.2.10}
$$
对于两个随机变量$$x,y$$，如果满足$$x=g(y)$$，变量$$x$$落在$$(x, x+\delta x)$$的概率对应于变量$$y$$落在$$(y,y+\delta y)$$的概率，即$$p_x(x)\delta x \simeq p_y(y)\delta y$$，可以得到$$p_y(y)$$与$$p_x(x)$$分别的表达式
$$
\begin{aligned}
p_y(y)=p_x(x)\left|\frac{dx}{dy}\right|=p_x(g(y))\left|g'(y)\right| \\
p_x(x)=p_y(y)\left|\frac{dy}{dx}\right|=p_y(g^{-1}(x))\left|{g^{-1}}'(x)\right| 
\end{aligned}
\tag{1.2.11}
$$
定义**累积分布函数（cumulative distribution function）**如下所示
$$
P(z) = \int_{-\infin}^z p(x)dx \tag{1.2.12}
$$
存在一个D维随机变量$$\mathbf{x}=(x_1,x_2,...x_D)$$，定义联合概率密度$$p(\mathbf{x})=p(x_1,...x_D)$$，其中落在包含$$\mathbf{x}$$的无限小体积$$\delta\mathbf{x}$$的概率为$$p(\mathbf{x})\delta{\mathbf{x}}$$。

$$p(\mathbf{x})$$满足非负性和归一性
$$
\begin{gathered}
p(\mathbf{x}) \ge 0 \\
\int p(\mathbf{x})d\mathbf{x} = 1 
\end{gathered}
\tag{1.2.13}
$$
离散型变量下的$$p(x)$$一般叫做**概率质量函数(probability mass function)**。离散随机变量情况下的加法法则和乘法法则叶同样适用于连续的情况
$$
p(x)=\int p(x,y)dy \tag{1.2.14}
$$

$$
p(x,y)=p(y|x)p(x) \tag{1.2.15}
$$

#### 1.2.2 期望和协方差（Expectations and covariances）

定义$f(x)$在概率分布$p(x)$下所取得的平均值为$f(x)$在分布$p(x)$下的**期望（expectation）**，表示为$\mathbb{E}[f]$，离散型和连续型的表达式分别为(1.2.16)和(1.2.17)
$$
\mathbb{E}[f] = \sum_x p(x)f(x) \tag{1.2.16}
$$

$$
\mathbb{E}[f] = \int p(x)f(x)dx \tag{1.2.17}
$$

从频率学的角度来看，如果在该概率分布下随机选取N个样本点，期望可以根据频率进行近似估计
$$
\mathbb{E}[f] \simeq \frac{1}{N}\sum_{n=1}^{N}f(x_n) \tag{1.2.18}
$$
对于含有多个随机变量的函数而言，可以对每个变量单独求期望， $f(x,y)$对变量$x$的期望可以写作
$$
\mathbb{E}_x[f(x,y)] \tag{1.2.19}
$$
定义$f(x,y)$ 关于$y$在分布$x$下的**条件期望（conditional expectation）**为
$$
\begin{gathered}
\mathbb{E}_x[f|y]=\sum_xp(x|y)f(x) \\
\mathbb{E}_x[f|y]=\int p(x|y)f(x) \\
\end{gathered}
\tag{1.2.20}
$$
定义$f(x)$的**方差（variance）**为
$$
\mathrm{var}[f]=\mathbb{E}[(f(x)-\mathbb{E}[f(x)])^2] \tag{1.2.21}
$$
展开后可以得到方差的另一种表达形式
$$
\mathrm{var}[f] = \mathbb{E}[f(x)^2]-\mathbb{E}[f(x)]^2 \tag{1.2.22}
$$
对于随机变量$x$自身的方差，对应于$f(x)=x$的情况，即
$$
\mathrm{var}[x]=\mathbb{E}[x^2]-\mathbb{E}[x]^2 \tag{1.2.23}
$$
对于两个随机变量$x,y$，定义其**协方差（covariance）**为
$$
\begin{aligned}
\mathrm{cov}[x,y]
&=\mathbb{E}_{x,y}[\{x-\mathbb{E}[x]\}\{y-\mathbb{E}[y]\}] \\
&=\mathbb{E}_{x,y}[xy]+\mathbb{E}_{x,y}[\mathbb{E}[x]\mathbb{E}[y]]-\mathbb{E}_{x,y}[x\mathbb{E}[y]] - \mathbb{E}_{x,y}[y\mathbb{E}[x]] \\
&=\mathbb{E}_{x,y}[xy]-\mathbb{E[x]}\mathbb{E[y]}
\end{aligned}
\tag{1.2.24}
$$
协方差一般用来表示变量之间的相关性，若变量之间相互独立，则协方差等于0。对于两个多维随机变量$\mathbf{x},\mathbf{y}$，定义协方差矩阵为
$$
\begin{aligned}
\mathrm{cov}[\mathbf{x},\mathbf{y}]
&=\mathbb{E}_{\mathbf{x},\mathbf{y}}[\{\mathbf{x}-\mathbb{E}[\mathbf{x}]\}\{\mathbf{y}^\top-\mathbb{E}[\mathbf{y}^\top] \}] \\
&=\mathbb{E}_{\mathbf{x},\mathbf{y}}[\mathbf{x}\mathbf{y}^\top]-\mathbb{E}[\mathbf{x}]\mathbb{E}[\mathbf{y}^\top]
\end{aligned}
\tag{1.2.25}
$$
向量$\mathbf{x}$的各个元素之间的协方差可以通过$\mathbf{x}$与自身的协方差矩阵来描述
$$
\mathrm{cov}[\mathbf{x}] \equiv \mathrm{cov}[\mathbf{x}, \mathbf{x}] \tag{1.2.26}
$$

#### 1.2.3 贝叶斯概率（Bayesian probabilities）

在观察到数据之前，我们有一些关于模型参数$\mathbf{w}$的假设，这以**先验概率（prior probability）**$p(\mathbf{w})$的形式给出。模型参数对观测数据$\mathcal{D}=\{t_1,...,t_N\}$的效果可以用条件概率$p(\mathbf{w}|\mathcal{D})$表达。贝叶斯定理的形式为
$$
p(\mathbf{w}|\mathcal{D})=\frac{p(\mathcal{D}|\mathbf{w})p(\mathbf{w})}{p(\mathcal{D})} \tag{1.2.27}
$$
它让我们能够通过**后验概率(posterior probability)**$p(\mathbf{w}|\mathcal{D})$，在观测到$\mathcal{D}$之后估计$\mathbf{w}$的不确定性。

$p(\mathcal{D}|\mathbf{w})$由观测数据集D来估计，可以被看成参数向量$\mathbf{w}$的函数，被称为**似然函数(likelihood function)**。给定似然函数的定义，贝叶斯定理可以被描述为
$$
\mathrm{posterior} \propto \mathrm{likelihood} \times \mathrm{prior} \tag{1.2.28}
$$
贝叶斯定理形式(1.2.27)的分母是一个归一化常数，确保后验概率的分布是一个合理的概率密度，积分为1。
$$
\begin{gathered}
\int p(\mathbf{w}|D)d\mathbf{w}=\int \frac{p(\mathcal{D}|\mathbf{w})p(\mathbf{w})}{p(\mathcal{D})}d\mathbf{w}=1 \\
p(\mathcal{D})=\int p(\mathcal{D}|\mathbf{w})p(\mathcal{\mathbf{w}})
\end{gathered}
\tag{1.2.29}
$$

#### 1.2.4 高斯分布

对于一元实值变量$x$，高斯分布被定义为
$$
\mathcal{N}(x|\mu,\sigma^2)=\frac{1}{(2\pi\sigma^2)^\frac{1}{2}}\exp\{-\frac{1}{2\sigma^2}(x-\mu)^2\} \tag{1.2.30}
$$
$\mu$叫做**均值(mean)**，$\sigma^2$叫做**方差(variance)**。方差的平方根，由$\sigma$给定，被叫做**标准差(standard deviation)**。方差的倒数，记作$\beta=\frac{1}{\sigma^2}$ ，被叫做**精度 **

高斯分布的非负性和归一性
$$
\mathcal{N}(x|\mu,\sigma^2)>0 
\tag{1.2.31}
$$

$$
\int_{-\infin}^{\infin}\mathcal{N}(x|\mu,\sigma^2)=1 \tag{1.2.32}
$$

高斯分布关于$x$的期望为
$$
\begin{aligned}
\mathbb{E}[x]
&=\int_{-\infin}^{\infin}\mathcal{N}(x|\mu,\sigma^2)xdx \\
&=\int_{-\infin}^{\infin}\mathcal{N}(x|\mu,\sigma^2)(x-\mu)d(x-\mu) + \mu\int_{-\infin}^{\infin}\mathcal{N}(x|\mu,\sigma^2)dx \\
&=\int_{-\infin}^{\infin}\frac{1}{\sqrt{2\pi}\sigma}\exp\{-\frac{(x-\mu)^2}{2\sigma^2}\}d(\frac{(x-\mu)^2}{2}) + \mu \\
&= \mu
\end{aligned}
\tag{1.2.33}
$$
高斯分布的二阶矩为
$$
\begin{aligned}
\mathbb{E}[x^2]
&=\int_{-\infin}^{\infin}\mathcal{N}(x|\mu,\sigma^2)x^2dx \\
&=\mu^2+\sigma^2
\end{aligned}
\tag{1.2.34}
$$

高斯分布的方差为
$$
\begin{aligned}
\mathrm{var}[x]
&=\mathbb{E}[(x-\mathbb{E}[x])^2] \\
&=\mathbb{E}[x^2]-\mathbb{E}[x]^2 \\
&=\sigma^2
\end{aligned}
\tag{1.2.35}
$$
定义高斯分布的$D$维形式为
$$
\mathcal{N}(\mathbf{x}|\pmb{\mu},\pmb{\Sigma})=\frac{1}{(2\pi)^{\frac{D}{2}}}\frac{1}{|\pmb{\Sigma}|^\frac{1}{2}}\exp\{-\frac{1}{2}(\mathbf{x}-\pmb{\mu})^T\pmb{\Sigma}^{-1}(\mathbf{x}-\pmb{\mu}) \} \tag{1.2.36}
$$
其中$D$维向量$\pmb{\mu}$被称为均值，$D\times D$的矩阵$\pmb{\Sigma}$被称为协方差，$|\pmb{\Sigma}|$表示$\pmb{\Sigma}$的行列式。

设$\mathsf{x}=(x_1,...,x_N)$是一组观测的数据集，表示变量$x$的$N$次观测，假定观测都是**独立同分布(i.i.d)**从高斯分布中$\mathcal{N}(x|\mu,\sigma^2)$抽取的，可以通过事件独立性的性质将概率累乘得到数据集的概率。
$$
p(\mathsf{x}|\mu,\sigma^2)=\prod_{n=1}^{N}\mathcal{N}(x_n|\mu,\sigma^2) \tag{1.2.37}
$$
如果把这个这个概率看作关于$\mu,\sigma^2$函数，则该函数称为高斯分布的**似然函数(Likelihood function)**

将上式取对数，得到**对数似然函数**
$$
\begin{aligned}
\ln p(\mathsf{x}|\mu,\sigma^2)
&=\sum_{n=1}^{N}\{-\frac{(x_n-\mu)^2}{2\sigma^2}+\ln\frac{1}{\sqrt{2\pi}\sigma}\} \\
&=-\frac{1}{2\sigma^2}\sum_{n=1}^{N}(x_n-\mu)^2 - \frac{N}{2}\ln\sigma^2 - \frac{N}{2}\ln2\pi
\end{aligned}
\tag{1.2.38}
$$
对于$\mu$而言，最大化对数似然函数可以得到$\mu$的最大似然解
$$
\mu_{ML}=\frac{1}{N}\sum_{n=1}^{N}x_n \tag{1.2.39}
$$
同理，方差$\sigma^2$的最大似然估计为
$$
\sigma^2_{ML} = \frac{1}{N}\sum_{n=1}^{N}(x_n-\mu_{ML})^2 \tag{1.2.40}
$$
最大似然估计对$\mu$的估计是无偏的，但对$\sigma^2$的估计是偏低的
$$
\mathbb{E}[\mu_{ML}]=\frac{1}{N}\sum_{n=1}^{N}\mathbb{E}[x_n]=\mu \tag{1.2.41}
$$

$$
\mathbb{E}[\sigma_{ML}^2]=(\frac{N-1}{N})\sigma^2 \tag{1.2.42}
$$

调整(1.2.40)可以得到对于$\sigma^2$的一个无偏估计量
$$
\widetilde{\sigma}^2=\frac{N-1}{N}\sigma_{ML}^2=\frac{1}{N-1}\sum_{n=1}^{N}(x_n-\mu_{ML})^2 \tag{1.2.43}
$$

#### 1.2.5 再看曲线拟合（Curve fitting re-visited）

曲线拟合问题是根据输入$\mathsf{x}=(x_1,...,x_N)^T$，和目标值$\mathsf{t}=(t_1,...t_N)^T$的来预测新输入变量的输出值的问题。如果从概率的角度重新考虑曲线拟合问题，假定对于给定$x$的值，对应的$t$服从高斯分布，分布的均值为$y(x,\mathbf{w})$，分布的精度为$\beta$，得到$t$的概率分布
$$
p(t|x,\mathbf{w},\beta)=\mathcal{N}(t|y(x,\mathbf{w}),\beta^{-1}) \tag{1.2.44}
$$
根据$\{\mathsf{x},\mathsf{t}\}$可以得到关于$\mathbf{w},\beta$的似然函数
$$
p(\mathsf{t}|\mathsf{x},\mathbf{w},\beta)=\prod_{n=1}^{N}\mathcal{N}(t_n|y(x_n,\mathbf{w}),\beta) \tag{1.2.45}
$$
对应的对数似然函数为
$$
\begin{aligned}
\ln p(\mathsf{t}|\mathsf{x},\mathbf{w},\beta)
&=\sum_{n=1}^{N}\ln\mathcal{N}(t_n|y(x_n,\mathbf{w}),\beta) \\
&=\sum_{n=1}^{N}\{\ln\sqrt{\frac{\beta}{2\pi}}-\frac{\beta(t_n-y(x_n,\mathbf{w}))^2}{2}\} \\
&=-\frac{\beta}{2}\sum_{n=1}^{N}\{y(x_n,\mathbf{w})-t_n\}^2 + \frac{N}{2}\ln \beta - \frac{N}{2}\ln 2\pi
\end{aligned}
\tag{1.2.46}
$$
可以得到对参数的最大似然估计
$$
\begin{gathered}
\mathbf{w}_{ML}=\arg\min_\mathbf{w}\sum_{n=1}^{N}\{y(x_n,\mathbf{w})-t_n\}^2 \\
\frac{1}{\beta_{ML}}=\frac{1}{N}\sum_{n=1}^{N}\{y(x_n,\mathbf{w}_{ML})-t_n\}^2 
\end{gathered}
\tag{1.2.47}
$$
确定了参数，可以通过给出$t$的概率分布的**预测分布(predictive distribution)**来进行预测
$$
p(t|x,\mathbf{w}_{ML}, \beta_{ML})=\mathcal{N}(t|y(x,\mathbf{w}_{ML}),\beta_{ML}^{-1}) \tag{1.2.48}
$$
引入$\mathbf{w}$的先验分布，假定先验分布为各向同性的高斯分布
$$
\begin{aligned}
p(\mathbf{w}|\alpha)
&=\mathcal{N}(\mathbf{w}|\mathbf{0},\alpha^{-1}\mathbf{I}) \\
&=\frac{1}{(2\pi)^\frac{M+1}{2}}\frac{1}{|\alpha^{-1}\mathbf{I}|^\frac{1}{2}}\exp\{-\frac{1}{2}\mathbf{w}^T(\alpha^{-1}\mathbf{I})^{-1}\mathbf{w}\} \\
&=(\frac{\alpha}{2\pi})^{\frac{M+1}{2}}\exp\{-\frac{\alpha}{2}\mathbf{w}^T\mathbf{w}\}
\end{aligned}
\tag{1.2.49}
$$
其中$\alpha$是分布的精度，$M+1$是对于$M$阶多项式的参数向量$\mathbf{w}$的元素的总数

根据贝叶斯定理，$\mathbf{w}$的后验概率正比于$\mathbf{w}$的先验概率与似然函数的乘积
$$
p(\mathbf{w}|\mathsf{x},\mathsf{t},\alpha,\beta) \propto p(\mathsf{t}|\mathsf{x},\mathbf{w},\beta)p(\mathbf{w}|\alpha)
$$
通过最大化后验概率$p(\mathbf{w}|\mathsf{x},\mathsf{t},\alpha,\beta)$的方法确定$\mathbf{w}$的方法叫做**最大后验法(maximum posterior,i.e. MAP)**
$$
\begin{aligned}
\mathbf{w}_{MAP}
&=\arg\max_\mathbf{w}p(\mathbf{w}|\mathsf{x},\mathsf{t},\alpha,\beta) \\
&=\arg\max_\mathbf{w}\ln p(\mathbf{w}|\mathsf{x},\mathsf{t},\alpha,\beta) \\
&=\arg\max_\mathbf{w}\{\ln p(\mathsf{t}|\mathsf{x},\mathbf{w},\beta)+\ln p(\mathbf{w}|\alpha)\} \\
&=\arg\max_\mathbf{w}\{-\frac{\beta}{2}\sum_{n=1}^{N}\{y(x_n,\mathbf{w})-t_n\}^2-\frac{\alpha}{2}\mathbf{w}^T\mathbf{w}\} \\
&=\arg\min_\mathbf{w}\{\frac{\beta}{2}\sum_{n=1}^{N}\{y(x_n,\mathbf{w})-t_n\}^2+\frac{\alpha}{2}\mathbf{w}^T\mathbf{w}\} \\
\end{aligned}
\tag{1.2.50}
$$
可以看到，最大化后验概率等价于最小化正则化的平方和误差函数，正则化参数为$\lambda=\frac{\alpha}{\beta}$

#### 1.2.6 贝叶斯曲线拟合（Bayesian curve fitting）

设训练数据为$\mathsf{x},\mathsf{t}$，新的测试点$x$，预测目标为$t$，曲线拟合的问题即为预测分布$p(t|x,\mathsf{x},\mathsf{t})$的问题。上述方法仅仅是对$\mathbf{w}$作点估计，而贝叶斯方法使用概率的加和规则和乘积规则，预测分布$p(t|x,\mathsf{x},\mathsf{t})$可以表示为如下对$\mathbf{w}$积分形式
$$
p(t|x,\mathsf{x},\mathsf{t})=\int p(t|x,\mathbf{w})p(\mathbf{w|\mathsf{x},\mathsf{t}})d\mathbf{w} \tag{1.2.51}
$$
对上式进行解析的求解，可以得到
$$
\begin{aligned}
p(t|x,\mathsf{x},\mathsf{t})
&=\int p(t|x,\mathbf{w})p(\mathbf{w}|\mathsf{x},\mathsf{t})d\mathbf{w} \\
&=\int p(t|x,\mathbf{w},\beta)p(\mathbf{w}|\mathsf{x},\mathsf{t},\alpha,\beta)d\mathbf{w} \\
&=\int p(t|x,\mathbf{w},\beta)\frac{p(\mathsf{t}|\mathsf{x},\mathbf{w},\beta)p(\mathbf{w}|\alpha)}{\int p(\mathsf{t}|\mathsf{x},\mathbf{w},\beta)p(\mathbf{w}|\alpha)d\mathbf{w}}d\mathbf{w} \\
&=\frac{\int p(t|x,\mathbf{w},\beta)p(\mathsf{t}|\mathsf{x},\mathbf{w},\beta)p(\mathbf{w}|\alpha)d\mathbf{w}}{{\int p(\mathsf{t}|\mathsf{x},\mathbf{w},\beta)p(\mathbf{w}|\alpha)d\mathbf{w}}} \\
&=\frac{\int \mathcal{N}(t|y(x,\mathbf{w}),\beta^{-1})\prod_{n=1}^{N}\mathcal{N}(t_n|y(x_n,\mathbf{w}))\mathcal{N}(\mathbf{w}|\mathbf{0},\alpha^{-1}\mathbf{I})d\mathbf{w}}{\int \prod_{n=1}^{N}\mathcal{N}(t_n|y(x_n,\mathbf{w}))\mathcal{N}(\mathbf{w}|\mathbf{0},\alpha^{-1}\mathbf{I})d\mathbf{w}} \\
&=\mathcal{N}(t|m(x),s^2(x))
\end{aligned}
\tag{1.2.52}
$$
回到曲线拟合问题，曲线拟合问题的$y(x,\mathbf{w})$定义为
$$
y(x,\mathbf{w})=\mathbf{w}^T\pmb{\phi}(x)=\sum_{i=0}^{M}w_i\phi_i(x)=\sum_{i=0}^{M}w_ix^i \tag{1.2.53}
$$
对于曲线拟合问题的预测分布的均值和方差分别为
$$
\begin{gathered}
m(x)=\beta\pmb{\phi}(x)^\mathrm{T}\mathbf{S}\sum_{n=1}^{N}\pmb{\phi}(x_n)t_n \\
s^2(x)=\beta^{-1}+\pmb{\phi}(x)^\mathrm{T}\mathbf{S}\pmb{\phi}(x) \\
\mathbf{S}^{-1}=\alpha\mathbf{I}+\beta\sum_{n=1}^{N}\pmb{\phi}(x_n)\pmb{\phi}(x_n)^\mathrm{T}
\end{gathered}
\tag{1.2.54}
$$

#### 练习

* 假设我们有三个彩色的盒子:$$r$$(红色)、$$b$$(蓝色)、$$g$$(绿色)。盒子$$r$$里有3个苹果，4个橘子，3个酸橙;盒子$$b$$里有1个苹果，1个橘子，0个酸橙;盒子$$g$$里有3个苹果，3 个橘子和4个酸橙。如果盒子随机被选中的概率为$$p(r) = 0.2，p(b) = 0.2，p(g) = 0.6$$。选择一个水果从盒子中拿走(盒子中选择任何水果的概率都相同)，那么选择苹果的概率是多少?如果 我们观察到选择的水果实际上是橘子，那么它来自绿色盒子的概率是多少? 
  $$
  \begin{aligned}
  p(apple)
  &=p(apple,r)+p(apple,b)+p(apple,g) \\
  &=p(apple|r)p(r)+p(apple|b)p(b)+p(apple|g)p(g) \\
  &=0.3*0.2+0.5*0.2+0.3*0.6 \\
  &=0.34
  \end{aligned}
  $$

  $$
  \begin{aligned}
  p(g|orange)
  &=\frac{p(orange|g)p(g)}{p(orange|g)p(g)+p(orange|b)p(b)+p(orange|r)p(r)} \\
  &=\frac{0.4* 0.6}{0.4*0.6+0*0.2+0.3*0.2} \\
  &=0.8
  \end{aligned}
  $$

  

* 考虑一个定义在连续变量$$x$$上的概率密度$$p_x(x)$$，假设我们使用x = g(y)做了一 个非线性变量变换，从而概率密度变换由公式(1.27)给出。通过对公式(1.27)取微分，请证 明，由于Jacobian因子的原因，y的概率密度最大的位置y􏰂与x的概率密度最大的位置x􏰂的关系通 常不是简单的函数关系x􏰂 = g(y􏰂)。这说明概率密度(与简单的函数不同)的最大值取决于变量 的选择。请证明，在线性变换的情况下，最大值位置的变换方式与变量本身的变换方式相同。 