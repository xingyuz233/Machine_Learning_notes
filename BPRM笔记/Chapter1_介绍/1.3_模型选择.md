### 1.3 模型选择（Model Selection）

#### 验证集与测试集

对于模型的评估，我们通常通过**验证集(validation set)**来比较不同模型的性能，或者不同参数下模型的性能，通过**测试集(test set)**来对模型做最后的评估。

#### 交叉验证法

由于训练数据和测试数据通常是很有限的，训练集过少会导致模型产生欠拟合，而验证集过少会对模型的评估产生一定的噪声。**交叉验证(cross-validation)**通过将数据分成$S$份，通过1份作为验证集，$S-1$份作为测试集，通过选取不同的那1份验证集进行$S$次的迭代，最终计算得到一个跟所有数据都相关的性能。当数据十分稀疏时，通常选取$S=N$，该情况下的交叉验证法又叫做**余一法(leave-one-out)**

但交叉验证训练的次数会随S呈线性的增加，并且当模型存在多个超参数时，训练次数会随超参数的个数呈指数式上升。下式表示对于同一个模型而言，交叉验证法下训练次数的影响因子
$$
\mathrm{times} \propto (S-1)*N*\#\mathrm{parameters}^{\mathrm{\#values}}
$$

#### 模型度量方法

为了减小过于复杂的模型造成的过拟合问题，我们需要重新找到一种模型表现的度量，一般的方法是增加一个惩罚项，尝试修正最大似然所造成的偏差。**赤池信息准则(Akaike information criterion, i.e. AIC)**选择下式作为模型表现的度量标准
$$
\ln p(\mathcal{D}|\mathbf{w}_{ML}) - M
$$
其中$p(\mathcal{D}|\mathbf{w}_{ML})$是最大化似然以后似然函数的值，$M$是模型超参数的个数，一种该准则的变体**贝叶斯信息准则(Bayesian information criterion, i.e. BIC)**将在后续的章节中进行讨论。


